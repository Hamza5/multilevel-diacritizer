{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### External libraries imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from random import randint\n",
    "import numpy as np\n",
    "from tensorflow import Variable, argmax, int32\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, TerminateOnNaN, TensorBoard, EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Internal imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from multilevel_diacritizer.model import MultiLevelDiacritizer\n",
    "from multilevel_diacritizer.metrics import DiacritizationErrorRate, WordErrorRate\n",
    "from multilevel_diacritizer.constants import (\n",
    "DEFAULT_EMBEDDING_SIZE, DEFAULT_LSTM_SIZE, DEFAULT_DROPOUT_RATE, DEFAULT_WINDOW_SIZE, DEFAULT_SLIDING_STEP,\n",
    "DEFAULT_BATCH_SIZE, DEFAULT_PARAMS_DIR, DEFAULT_MONITOR_METRIC, DEFAULT_EARLY_STOPPING_STEPS, DEFAULT_TRAIN_STEPS\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#@title Dataset files\n",
    "TRAIN_DATA_FILES = [str(p) for p in Path('data/tashkeela_train/').glob('tashkeela_train_*.txt')] #@param {type:\"raw\"}\n",
    "VAL_DATA_FILES = [str(p) for p in Path('data/tashkeela_val/').glob('tashkeela_val_*.txt')] #@param {type:\"raw\"}\n",
    "TEST_DATA_FILES = [str(p) for p in Path('data/tashkeela_test/').glob('tashkeela_test_*.txt')] #@param {type:\"raw\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construction of the model "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MultiLevelDiacritizer(window_size=DEFAULT_WINDOW_SIZE, lstm_size=DEFAULT_LSTM_SIZE,\n",
    "                              dropout_rate=DEFAULT_DROPOUT_RATE, embedding_size=DEFAULT_EMBEDDING_SIZE)\n",
    "model.summary(positions=[.45, .6, .75, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = MultiLevelDiacritizer.get_processed_window_dataset(\n",
    "            TRAIN_DATA_FILES, DEFAULT_BATCH_SIZE, DEFAULT_WINDOW_SIZE, DEFAULT_SLIDING_STEP\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_set = MultiLevelDiacritizer.get_processed_window_dataset(\n",
    "    VAL_DATA_FILES, DEFAULT_BATCH_SIZE, DEFAULT_WINDOW_SIZE, DEFAULT_SLIDING_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compiling the model and loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(RMSprop(0.001),\n",
    "                      [SparseCategoricalCrossentropy(from_logits=True, name='primary_loss'),\n",
    "                       SparseCategoricalCrossentropy(from_logits=True, name='secondary_loss'),\n",
    "                       BinaryCrossentropy(from_logits=True, name='shadda_loss'),\n",
    "                       BinaryCrossentropy(from_logits=True, name='sukoon_loss')])\n",
    "model_path = DEFAULT_PARAMS_DIR / Path(\n",
    "    f'{model.name}-E{DEFAULT_EMBEDDING_SIZE}L{DEFAULT_LSTM_SIZE}W{DEFAULT_WINDOW_SIZE}S{DEFAULT_SLIDING_STEP}.h5'\n",
    ")\n",
    "if model_path.exists():\n",
    "    print('Loading model weights from %s ...' % str(model_path))\n",
    "    model.load_weights(str(model_path), by_name=True, skip_mismatch=True)\n",
    "else:\n",
    "    print('Initializing random weights for the model %s ...' % model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "last_epoch_path = DEFAULT_PARAMS_DIR / Path('last_epoch.txt')\n",
    "\n",
    "def write_epoch(epoch, logs):\n",
    "    with last_epoch_path.open('w') as f:\n",
    "        print(epoch, file=f)\n",
    "        print(logs, file=f)\n",
    "\n",
    "def get_initial_epoch():\n",
    "    if last_epoch_path.exists():\n",
    "        with last_epoch_path.open() as f:\n",
    "            return int(f.readline())\n",
    "    return 0\n",
    "\n",
    "def get_diacritization_preview(val_set, sliding_step, model, limit):\n",
    "    x, (pri, sec, sh, su) = next(iter(\n",
    "            val_set['dataset'].skip(randint(1, val_set['size'] - 1)).take(1)\n",
    "        ))\n",
    "    x, pri, sec, sh, su = x[:limit], pri[:limit], sec[:limit], sh[:limit], su[:limit]\n",
    "    predicted = model.predict_sentence_from_input_batch(x, sliding_step).numpy().decode('UTF-8')\n",
    "    real = model.generate_real_sentence_from_batch(\n",
    "        (x, [pri, sec, sh, su]),\n",
    "        sliding_step\n",
    "    )\n",
    "    return predicted, real\n",
    "\n",
    "\n",
    "model.fit(train_set['dataset'].repeat(), steps_per_epoch=train_set['size'], epochs=DEFAULT_TRAIN_STEPS,\n",
    "          initial_epoch=get_initial_epoch(),\n",
    "          validation_data=val_set['dataset'].repeat(), validation_steps=val_set['size'],\n",
    "          callbacks=[ModelCheckpoint(str(model_path), save_best_only=True, save_weights_only=True,\n",
    "                                     monitor=DEFAULT_MONITOR_METRIC), TerminateOnNaN(),\n",
    "                     EarlyStopping(monitor=DEFAULT_MONITOR_METRIC, patience=DEFAULT_EARLY_STOPPING_STEPS, verbose=1),\n",
    "                     LambdaCallback(\n",
    "                         on_epoch_end=lambda epoch, logs: print(\n",
    "                             'Predicted diacritization: %s\\nReal diacritization: %s' %\n",
    "                             get_diacritization_preview(val_set, DEFAULT_SLIDING_STEP, model, 100)\n",
    "                         )\n",
    "                     ), LambdaCallback(on_epoch_end=write_epoch), TensorBoard()\n",
    "                     ]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = MultiLevelDiacritizer.get_processed_window_dataset(\n",
    "    TEST_DATA_FILES, DEFAULT_BATCH_SIZE, DEFAULT_WINDOW_SIZE, DEFAULT_SLIDING_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MultiLevelDiacritizer(window_size=DEFAULT_WINDOW_SIZE, lstm_size=DEFAULT_LSTM_SIZE,\n",
    "                              dropout_rate=DEFAULT_DROPOUT_RATE, embedding_size=DEFAULT_EMBEDDING_SIZE)\n",
    "model_path = DEFAULT_PARAMS_DIR / Path(\n",
    "    f'{model.name}-E{DEFAULT_EMBEDDING_SIZE}L{DEFAULT_LSTM_SIZE}W{DEFAULT_WINDOW_SIZE}S{DEFAULT_SLIDING_STEP}.h5'\n",
    ")\n",
    "if model_path.exists():\n",
    "    print('Loading model weights from %s ...' % str(model_path))\n",
    "    model.load_weights(str(model_path), by_name=True, skip_mismatch=True)\n",
    "else:\n",
    "    print('Weights file for the selected model is not found in %s. The model weights are initialized randomly.' % str(model_path.parent))\n",
    "\n",
    "der = Variable(0.0)\n",
    "wer = Variable(0.0)\n",
    "count = Variable(0.0)\n",
    "print('Calculating DER and WER...')\n",
    "for i, (x, diacs) in test_set['dataset'].enumerate(1):\n",
    "    pri_pred, sec_pred, sh_pred, su_pred = model(x)\n",
    "    pred_diacs = [\n",
    "        MultiLevelDiacritizer.combine_windows(argmax(v, axis=2, output_type=int32), DEFAULT_SLIDING_STEP)\n",
    "        for v in model(x)\n",
    "    ]\n",
    "    x = MultiLevelDiacritizer.combine_windows(x, DEFAULT_SLIDING_STEP)\n",
    "    diacs = [MultiLevelDiacritizer.combine_windows(v, DEFAULT_SLIDING_STEP) for v in diacs]\n",
    "    diacritics = MultiLevelDiacritizer.decode_encoded_diacritics(diacs)\n",
    "    pred_diacritics = MultiLevelDiacritizer.decode_encoded_diacritics(pred_diacs)\n",
    "    der.assign_add(1 - DiacritizationErrorRate.char_acc((diacritics, pred_diacritics, x)))\n",
    "    wer.assign_add(1 - WordErrorRate.word_acc((diacritics, pred_diacritics, x)))\n",
    "    count.assign_add(1)\n",
    "    print('Batch %d/%d: DER = %f | WER = %f' % (i, test_set['size'], (der / count).numpy(), (wer / count).numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}